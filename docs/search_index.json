[["index.html", "Duke EJ Lab Goals", " Duke EJ Lab The EJ Lab is an interdisciplinary group that works across the Kenan Institute for Ethics and the Nicholas Institute for Energy, Environment &amp; Sustainability to bring data tools and research justice principles to community building projects. The EJ Lab aims to provide community partners with access to data-driven research in the form that’s most useful to them - whether that be peer reviewed papers, policy briefs, or public use data tools. We are currently focused on examining environmental disparities in housing markets and neighborhoods, social and environmental determinants of health for communities, and the efficacy of environmental justice policy solutions. Goals "],["people.html", "People Alums", " People Leadership Kay Jowers - Director of the Just Environments Program in the Nicholas Institute for Energy, Environment &amp; Sustainability Chris Timmins - Professor of Economics Current Graduate RA’s Jaeyong (Danny) Han Liz Lu Sarah Raviola Yu Ma Current Undergrad RA’s Thomas Barker Wendy Shi Zhaoya Wu Zoe Macomber Alums Annabel (Qihui) Hu, 2021 - PhD Candidate @ CMU Heinz Bryan Montes, 2021 - Energy, Sustainability, and Infrastructure Consultant at Guidehouse Cassandra Turk, 2021 Julia Marshall, 2021 - Nehal Jain, 2021 - Ryan Hastings, 2021 - Econ PreDoc @ Wharton Real Estate Daisy (Xiaoou) Zhan, 2020 - PhD Candidate @ U Wisconsin School of Business Davis Berlind, 2020 - PhD Candidate @ UCLA Statistics Brian Wong, 2019 - MBA Candidate @ Duke Fuqua Evelyn Garcia, 2019 - Jules Carter, 2019 - Spencer Yu, 2019 - Xueting Pi, 2019 - PhD Candidate @ U of Maryland, Environmental Economics Anne Driscoll, 2018 - Staff RA @ EJ Lab Ryan Hoecker, 2018 - Senior Associate @ Social Finance "],["onboarding.html", "Onboarding Code Organization Data Storage ", " Onboarding Code Organization Code for lab projects all lives in the lab GitHub. Many of the repos discussed below are set to only be accessible to members of the GitHub lab group, so be sure to request access. Individual Code Organization All lab projects start from a reproducible project template. Starting all projects from this file structure gives a framework that helps make our data and code more uniform, helping with reproducibility and keeping institutional knowledge as students join and leave projects. This template is set up with example R files, but the same ideas apply regardless of the language you’re using. The template repo itself documents what should go in what folder, and includes examples of documentation and commenting throughout code files. An example of the template in use is the COVID housing precarity repo. Some external examples for how a publishable repo can look like: a paper called The Changing Risk and Burden of Wildfire in the United States, . Lab Code Organization There are two communal code repositories to know about: The lab.tools repo, which is an R package that everyone can add to and use in their code. You can find more information about how to contribute in the workflow section. It contains some helpful little functions like detach_all_packages() that lets you quickly wipe all the loaded functions from your environment - as well as some helpful analysis functions like merge_verbose() which wraps the normal merge function but outputs some info on the success of the merge or simplify_and_combine_shapefiles() which takes in a vector of file paths, simplifies the shapefiles and returns a master file, which is useful when data is served at a state level. The general_cleaning repo, which serves to ensure we know the origin of all our communal lab data. There’s more information about how to interact with the repo in the workflow section. Data Storage Lab data is stored only be in Duke Box, or one of the lab VM’s - not a personal computer, not dropbox, not a hard drive under the desk in Kay’s office. Box The EJ Lab Box has three main folders, projects, clean, and raw. raw is effectively a read-only folder. Data is added relatively rarely and it’s important when things are added to the raw folder to document well when the data was downloaded, how to obtain the data (a link to download/an email for who to contact to get access), and what the data means (a data dictionary). More details about the exact way to do this are in the workflow section. clean holds processed data that you might want to directly pull into a project. For something to get added to clean a few conditions need to be met: the raw data is in the raw folder, and the code that cleans it is in the lab cleaning github. This is only for data that might be useful for someone else on a different lab project. A dataset to include might be: the average PM for every year 2000-2020 for each county in the contiguous US. A dataset not to include might be: a panel that includes 21 day rolling mean county level PM observation that you built to purpose for a model. projects is the folder that contains a folder for each active or archived project. While we’d recommend a structure that at least separates your raw data, clean data and any docs files you have (eg literature reviews, research updates, other notes to self, etc) - this is a space you can do whatever you please with. Below is a minimal representation of what the Box structure looks like. ejlab ├── projects │ ├── covid_housing_precarity │ ├── covid_rental_discrimination │ └── cafos_nc ├── clean │ ├── rsei │ ├── polidata │ └── pm25 └── raw ├── rsei ├── polidata └──pm25 VM’s There are a few datasets that should only exist on Duke owned machines (the VM’s) or (if they have even higher protections) that can only exist in the protected research data network (PRDN). Information on getting access to those spaces can be found in the onboarding section. This includes the original datasets, and ANY data that is created from it/combined with it/modeled using it, excluding simple tables that might be need to go in a publication. The datasets currently used in lab that these protections apply to are: InfoUSA Corelogic "],["workflow.html", "Workflow Starting up a Project Code Documentation Documenting Data Closing out a Project", " Workflow This workflow section will cover a lot of expectations in pretty high detail and may be slightly overwhelming. The first section covers the immediate steps that you need to know as you go out, so feel free to start there and come back to this over time. If you find anything unclear feel free to edit this in the repository. Starting up a Project Make sure you have access to everything you need: Duke Box (talk to Kay) and the Duke-EJ-Lab GitHub, the VM’s (talk to Anne or Kay, may take a little while) if you are working with protected data or need lots of computational power; make sure you have all the software you need installed: Github Desktop, R, and RStudio or Python and your prefered IDE. We use Github Desktop in this walkthrough for ease, but you’re welcome to use git in command line - whatever fits your needs! Create a GitHub repo to store all your work in that’s within the Duke-EJ-Lab, based off the template. At the reproducible_project_template page, hit the green “Use this template” button and choose a repository name that’s descriptive of your project, separating words with “_“. Set it to private while the work is in progress, so only other lab members can see the repository. With GitHub Get a local version of your GitHub repo. In GitHub Desktop, find “File -&gt; Clone Repository”, which will bring up a list of repos you could have locally. Select the GitHub repo you just created (you may need to hit the refresh button if you just created it) and hit the blue “Clone” button at the bottom right. This will create a folder containing all the example structure files in your local Documents/GitHub folder, you can make any edits to the documents in those folders. Create a data folder inside the projects folder of the Duke Box with the same name as you chose for your GitHub repository. This is going to hold all the data that is unique to your project - that covers any data sources that aren’t likely to be helpful for future projects or any of your processed data. It’s recommended you follow something similar to the structure shown here, where you separate the raw and clean data within your project folder. Add the data you need to your Box project folder. Add documentation on that data to your Box. It’s most important to document the raw data that you download, so that anyone coming in to your code at a later date will be able to understand the context of the data, know how to update it, and be able to find documentation from the data originator. Add to the README in your Box folder at a minimum when the data was downloaded, how to obtain the data (a link to download/an email for who to contact to get access), and what the data means (a data dictionary) if the data dictionary is not readily accessible either in your folder or at the link to download the data. Here’s an example of of what the README might look like: a layout of the folder structure, followed by the when/how/what of each file. This is an iterative process! Every time you need new data, you’ll come back to update the data documentation. Write some code. Make sure to save even the exploratory data analysis (EDA) work you’re doing - there’s an eda folder in your GitHub repository that you can save out intermediate files in. Commit your code to GitHub. Any time you make a change to your code, commit the change to the online version of the code. It’s especially important to do this very often if you are collaborating with others. Good times to commit are: if you just solved a problem you’ve been working on, or if you’re going to stop actively working on it (eg going to class, working on a paper). To commit, go to GitHub Desktop and add a commit message that explains what you’re adding, then hit the blue “Commit” button on the bottom left. Once you’ve done that, hit the black “Push origin” button on the top right. Comment, document, and iterate! Go back and forth between writing code, committing it, and cleaning up your code + folder structure as you work through the project - it’s inevitable you’ll be trying different methods, different data sources, finding issues that makes you go through old work, and you want to keep the documentation as clean and minimal as you can throughout the process. Keep any analysis that informed your ultimate decisions and make sure it’s commented, but drop code and data that’s no longer relevant. Code Documentation Commenting Every code file should start with something that looks like this and contains: the creator who else is working/has worked on this file when it was last edited a one or two sentence description of what the file is trying to do what the output of the file is. ######################################################################################## # Created by: Anne Driscoll # Other contributors: Chris Timmins, Kay Jowers # Last edited on: 8/31/22 # # This file creates a cleaned dataset of all US flights that is used throughout # the rest of the analysis in this project. # # In the cleaned data each observation is an airport-year, and gives total number of # flights, along with taxi time and several delay metrics. ######################################################################################## If the file is tackling more than one task at a time, it helps to break up the sections for readability, so that others can follow what the code is doing. That might look something like this: # Load data -------------------------------------------------------------------- # Filter data to only TX airports ---------------------------------------------- # Plot number of flights over time --------------------------------------------- Blocks of code should include more detailed commenting throughout, so that your collaborators can follow your code without spending too much time looking through the details of your code and the functions you call - reduce the mental load! The most important kinds of comments are those that explain why you’re implementing something. A quick explanation of the universe of observations you are aiming for, a mention of the theoretical basis for why you’re defining a variable a specific way, can be really useful so people understand the research decisions you’re making. File Naming Good examples would be process_pm_data.R, identify_rural_homes.R, or create_figure_2.R. Bad examples would be CAFO2trylm.R, analysis.R, or cleaning up the obs from corelogic w rural designation.R. Most of the work we do will be a bunch of ordered files, and these should be numbered. Using the examples above, they should be named in order - 01_process_pm_data.R and 02_create_figure_2.R (assuming no files in between!). These can always be changed if you need to add steps in between! Object naming Variable and function names should be lowercase, and use an underscore to separate words. Try to be concise! Good examples would be day_one or first_day, bad examples would be first_day_in_the_data or dotm1. This is largely lifted from Hadley Wickham’s style guide. Additional guidance can be found there, but these are the most important parts for us. Documenting Data Ever piece of data in the raw folder needs to have documented the what, when, how of the data. Closing out a Project Go through your code and comment well Go through your code and make sure you’re using it all, trim down whatever isn’t part of the project anymore. EG are you still using the three different metrics you built? Are you using both versions of data on the same topic? go through the data you used and decide if anything in your ejlab/project folder might actually be something that generalizes to other lab projects. If it does, move the raw data to the ejlab/raw folder (or a relevant subfolder) and document the what, when, how of the data. If there’s a clean version, hand off to a team mate and make sure they can run Contributing to the lab data bank Contributing to the lab R package The lab R package is currently really minimal, but contains some functions that are useful to some frequent data cleaning tasks that come up across lab projects. If you’re writing a block of code that does a similar task over and over, consider if that’s something that might be useful to just call as a function on your data, and feel free to add it to the lab package! If you do, add a bullet to the lab package section of this document! A step by step guide on how to contribute to the lab R package can be found in the package repo itself! Contributing to (this!) lab onboarding doc Find the GitHub repo that contains all the documents for this site, and get it locally. Install the bookdown package. install.packages(&quot;bookdown&quot;) library(bookdown) Make changes to whichever of the Rmd files you wanted to update. The Rmd files aren’t what’s actually displayed on the site, it’s actually the html files that are created from the Rmd files. To update the html files, use the bookdown package to render the book. render_book() Commit (and push) your changes, so they are stored on the GitHub website instead of just your local computer, and the changes will show up automatically. Beware it can take a little while for changes to actually show up. "],["data-library.html", "Data Library Environmental Data Social Data Other", " Data Library Environmental Data RSEI What: Where: Why: PM 2.5 Spatial Resolution + Coverage: The PM 2.5 data that lives in the Box is in a raster format, one of the finest resolution datasets (0.01° × 0.01°) built for PM 2.5. The data we have pre-loaded is specifically for North America, and only covers the contiguous 48, if you need Alaska and Hawaii, or international data you’ll need to download their Global/Regional Estimates (V5.GL.02). Temporal Resolution + Coverage: The PM 2.5 data that lives in Box currently covers 2018-2000 and is at a yearly average level. Formats: The raw raster data lives in the Box raw folder. There is also some processed data in the clean/pm25 folder, at state, county and tract level where each observation is a yearly average as aggregated over that spatial unit (using weighted average, weighting by the area of the grid cell covered by the spatial unit), only including the lower 48. Source: Information on the different products can be found here. We specifically have downloaded the Social Data InfoUSA Corelogic Other US boundary shapefiles "],["tools-to-know-about.html", "Tools to know about Basic packages to know about (R) Cool packages to know about (R) Cool functions to know about (R) Basic packages to know about (Python) Cool packages to know about (Python) Cool functions to know about (Python) Other random cool stuff", " Tools to know about Basic packages to know about (R) data.table: hard to read, but faster on average data manipulation raster: work with raster data rmarkdown: the R notebook interface, useful for saving EDA with explanation or for reports, but generally not used for heavy lifting final code. sf: spatial data, more flexible sp: spatial data, more built in options tidyverse dplyr: “a grammar of data manipulation” ggplot2: quick and easy graphics lubridate: easier date/time formats readr: fast and easy way to read flat files stringr: common string manipulations tidyr: reshaping data Cool packages to know about (R) bookdown: lets you build websites and “books” really easily (how this site is built). caret: streamlined model building (Classification And REgression Training) that handles data splitting, pre processing, feature selection, and model tuning and selection. conflicted: alternate way of dealing with package naming conflicts. Great if you’ve ever gotten an error like Unable to find an inherited method for function ‘select’ for signature ‘\"data.frame\"’ because you recently loaded a package that covers a function you commonly use. datapasta: if you ever copy and pasta data from an online table that you later use in your research you should be using this. It lets you copy and paste data directly into an R object, that can then be saved into your reproducible workflow. exactextractr: helps you use rasters and shapefiles to figure out the values in the raster within the shapefile area. Gives lots of control over how you extract - aggregating functions, weighting each raster cell, etc. feather: a binary data format that is language agnostic with high write and read performance. patchwork: lets you arrange several ggplot plots into table like configurations. pbapply: the classic lapply function, but provides a progress bar! profvis: gives you a graphical representation of how quickly each line of a block of code is running. A great tool when something is running slower than expected but you don’t really know where it’s going wrong. ranger: fast random forests! reticulate: the R interface to Python. roxygen: critical for package development. Learn it by contributing to the lab package! rvest: scrape the internet! sqldf: another way to manipulate data in R. sometimes useful since it uses SQLite as the underlying database, so for large data it is usually much faster than straight R. Great for an initial read-in filter. tidycensus: work with Census data directly from your R code. You give it the variable titles and years you want and it pulls it from the Census API. Great for reproducibility! tidytext: text mining wesanderson: color palletes based on Wes Anderson’s movies, some of them make some really beautiful plots. (Personally I love Zissou1, Darjeeling1 and GrandBudapest1 -Anne) validate: great to do checks on your final panel before modeling, makes it easy to “check whether data lives up to expectations you have based on domain knowledge”. A great thing to build into a final code package - also helps people who come back to your code what to expect. vroom: frequently the fastest way to read data in to R. readr has a throughput of 44.02 MB/sec, while vroom reaches 1.23 GB/sec. Our lab package Our lab package: an R package with a mismash of random helpful or fun functions. A sample of functions below. Anyone can add to the package, and it’s great to learn how to build a package!! Any useful functions you stumble upon can be added here (but make sure to include credit to the creator). detach_all_packages: quickly wipes the packages loaded to your R environment, useful for ensuring reproducibility (since the packages you have loaded impact how things run!) beep_alert: a function that tells you aloud that your code has finished running. Useful when you have things running just long enough that you want to go look at your email or something. merge_verbose: wraps the base merge function, but spits out some info on your merge to the console. some extra ggplot themes Cool functions to know about (R) file.path(): a function that lets you reference paths without building the path manually (eg user/anne/data/testing vs file.path(\"user\", \"anne\", \"data\", \"testing\")), which is great because it means your code won’t break when it crosses to a different OS. Hmisc::describe(): sometimes more useful version of summary to peek into a dataset. It adds the number of distinct observations, as well as directly showing the highest and lowest observations. dplyr::complete(): a function that can help making implicit NA’s explicit. BAMMtools::getJenksBreaks(): great for plotting, given a vector it returns the “Jenks” breaks for cloropleth maps (commonly used in many GIS systems). Basically just returns a list of numbers that you can pass to a mapping function that specify where color changes should happen. Basic packages to know about (Python) NumPy: numerical computing, random number generation, etc Pandas: flexible data structures, data wrangling and cleaning Matplotlib: data visualization scikit-learn: classification, regression, clustering, and some cleaning Cool packages to know about (Python) BeautifulSoup: scraping Cool functions to know about (Python) Other random cool stuff How do we think about EDA and pipelines? blog personal R packages "],["general-stumbling-blocks.html", "General Stumbling Blocks Theoretical Stumbling Blocks Data Cleaning Stumbling Blocks Shapefile Stumbling Blocks", " General Stumbling Blocks Theoretical Stumbling Blocks Test/Train temporal? Data Leakage generally Data Cleaning Stumbling Blocks fail early and often - build in explicit data validation w/ validate::check_that Aggregating - what’s the best function? What are you trying to capture? Do you need to account for area? Do you need to account for population? Do you only care about aggregating over spaces that meet certain conditions? leading 0’s implicit NA’s make sure to use census block group/tract/county FIPS codes for a correct + consistent year Codes change from year to year, so if you use the 2010 set in one part of your code and the 2020 set in another part of your code you’ll drop data that you want to have, and add NA’s that are “false”. Shapefile Stumbling Blocks Projections If you have shapefiles in different projections, you may get unexpected (and incorrect) results when you extract or aggregate. Extracting accurately Extraction functions can reach pretty different answers depending on their methods! Is the function you’re using extracting only the pixel that represents the centroid of your shapfile? Is the function you’re using extracting only pixels that your shapefile completely covers? Is your shapefile weighting by the area of the pixel that your shapefile covers? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
