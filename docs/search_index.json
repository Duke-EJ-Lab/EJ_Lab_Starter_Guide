[["index.html", "Duke EJ Lab Goals", " Duke EJ Lab The EJ Lab is an interdisciplinary group that works across the Kenan Institute for Ethics and the Nicholas Institute for Energy, Environment &amp; Sustainability to bring data tools and research justice principles to community building projects. The EJ Lab aims to provide community partners with access to data-driven research in the form that’s most useful to them - whether that be peer reviewed papers, policy briefs, or public use data tools. We are currently focused on examining environmental disparities in housing markets and neighborhoods, social and environmental determinants of health for communities, and the efficacy of environmental justice policy solutions. Goals "],["people.html", "People Alums", " People Leadership Kay Jowers - Director of the Just Environments Program in the Nicholas Institute for Energy, Environment &amp; Sustainability Chris Timmins - Professor of Economics Current Graduate RA’s Jaeyong (Danny) Han Liz Lu Sarah Raviola Yu Ma Current Undergrad RA’s Thomas Barker Wendy Shi Zhaoya Wu Zoe Macomber Alums Annabel (Qihui) Hu, 2021 - PhD Candidate @ CMU Heinz Bryan Montes, 2021 - Energy, Sustainability, and Infrastructure Consultant at Guidehouse Cassandra Turk, 2021 Julia Marshall, 2021 - Nehal Jain, 2021 - Ryan Hastings, 2021 - Econ PreDoc @ Wharton Real Estate Daisy (Xiaoou) Zhan, 2020 - PhD Candidate @ U Wisconsin School of Business Davis Berlind, 2020 - PhD Candidate @ UCLA Statistics Brian Wong, 2019 - MBA Candidate @ Duke Fuqua Evelyn Garcia, 2019 - Jules Carter, 2019 - Spencer Yu, 2019 - Xueting Pi, 2019 - PhD Candidate @ U of Maryland, Environmental Economics Anne Driscoll, 2018 - Staff RA @ EJ Lab Ryan Hoecker, 2018 - Senior Associate @ Social Finance "],["onboarding.html", "Onboarding Lab structure Code Organization Data Storage Basics of GitHub", " Onboarding Lab structure The lab consists of two Principal Investigators (PIs): Chris Timmins (a professor in Econ) and Kay Jowers (the Director of the Just Environments program at the Nicholas Institute). There’s currently a staff Analyst, Anne Driscoll. Everybody meets once a week in person so we can all get a sense of what’s going on with everyone in the lab. Meetings start with a rose/bud/thorn check-in, followed by a presentation by a lab member, a lit review by another lab member, and a factoid or figure share by a third lab member. Mostly, each student is working on one project in collaboration with one or both of the PIs on a relatively long term basis. Research meetings between the lab PIs and students help define the structure and goals of the project, help to move research forward and troubleshoot things that come up throughout the process, but most work happens pretty independently. Code Organization Code for lab projects all lives in the lab GitHub. Many of the repos discussed below are set to only be accessible to members of the GitHub lab group, so be sure to request access. Individual Code Organization Each lab project will have it’s own GitHub repository, which should be reproducible by people within lab. All lab projects start from a reproducible project template. Starting all projects from this file structure gives a framework that helps make our data and code more uniform, helping with reproducibility and keeping institutional knowledge as students join and leave projects. This template is set up with example R files, but the same ideas apply regardless of the language you’re using. The template repo itself documents what should go in what folder, and includes examples of documentation and commenting throughout code files. An example of the template in use is the COVID housing precarity repo. External examples for how a publishable repo can look: The Changing Risk and Burden of Wildfire in the United States. Once you have access to the lab GitHub, you should be able to create repositories for each of your own projects. Details on that can be found in the section on starting a project. Lab Code Organization There are two communal GitHub code repositories to know about: The lab.tools repo, which is an R package that everyone can add to and use in their code. You can find more information about how to contribute in the workflow section. It contains some helpful little functions like detach_all_packages() that lets you quickly wipe all the loaded functions from your environment - as well as some helpful analysis functions like merge_verbose() which wraps the normal merge function but outputs some info on the success of the merge or simplify_and_combine_shapefiles() which takes in a vector of file paths, simplifies the shapefiles and returns a master file, which is useful when data is served at a state level. You can install the lab.tools package through first loading the devtools package, then running install_github(\"Duke-EJ-Lab/lab.tools\"). Then just load as a normal library using library(lab.tools). The general_cleaning repo, which serves to ensure we know the origin of all our communal lab data. There’s more information about how to interact with the repo in the workflow section. In addition to the code repositories, this EJ_Lab_Starter_Guide repo is something you can access and update. Details on how to do that are in the guide section. Data Storage Lab data is stored only be in Duke Box, or one of the lab VM’s - not a personal computer, not dropbox, not a hard drive under the desk in Kay’s office. To get access to both of these, talk to Kay. Box The EJ Lab Box has three main folders, projects, clean, and raw. raw is effectively a read-only folder. Data is added relatively rarely and it’s important when things are added to the raw folder to document well when the data was downloaded, how to obtain the data (a link to download/an email for who to contact to get access), and what the data means (a data dictionary). More details about the exact way to do this are in the workflow section. clean holds processed data that you might want to directly pull into a project. For something to get added to clean a few conditions need to be met: the raw data is in the raw folder, and the code that cleans it is in the lab cleaning github. This is only for data that might be useful for someone else on a different lab project. A dataset to include might be: the average PM for every year 2000-2020 for each county in the contiguous US. A dataset not to include might be: a panel that includes 21 day rolling mean county level PM observation that you built to purpose for a model. projects is the folder that contains a folder for each active or completed project. While we’d recommend a structure that at least separates your raw data, clean data and any docs files you have (eg literature reviews, research updates, other notes to self, etc) - this is a space each RA can do whatever they please with. Below is a minimal representation of what the Box structure looks like. ejlab ├── projects │ ├── covid_housing_precarity │ ├── covid_rental_discrimination │ └── cafos_nc ├── clean │ ├── boundaries │ ├── rsei │ ├── polidata │ └── pm25 └── raw ├── boundaries ├── rsei ├── polidata └── pm25 VM’s There are a few datasets that should only exist on Duke owned machines (the VM’s) or (if they have even higher protections) that can only exist in the protected research data network (PRDN). Information on getting access to those spaces can be found in the onboarding section. This includes the original datasets, and ANY data that is created from it/combined with it/modeled using it, excluding simple tables that might be need to go in a publication. The datasets currently used in lab that these protections apply to are: InfoUSA For projects that work with a protected dataset, most of the work will occur on the VM itself. This doesn’t change the workflow very much as all the tools we work through (Box and GitHub Desktop) are available through the VM as well. Basics of GitHub Though it’s much more complicated and has tons of more intricate use cases, at it’s most basic we can think of git as similar to Box. A file exists in Box in the cloud, and sometimes also exists on your local computer. You can make changes on your computer, and that can change what’s in the cloud (and therefore what’s on your friend’s computer). GitHub uses “repositories” (repos) as buckets that contain individual projects. Git is a little more manual than Box though! You have to manually let it know when to take action - and there are 3 actions you need to know about. A commit is basically like saving your progress. You’re telling git to pay attention and create a save point of what’s changed since you last committed. This is the building block of a repo, each commit should be a bite sized change in your work. A push is sending your commits to the cloud. Until you do this, your commits only exist on your computer, and if your colleagues look at your online GitHub repo nothing will have changed. A pull is telling Git to check if anyone else has changed the online repo, and updating what’s on your computer to match. That’s pretty much all you need to know to get started, and you’ll see more about commits and pushing in the next section. In our usage, we’ll always talk about commiting then immediately pushing afterwards, since not doing so can cause complications. From here on we’re describing GitHub through the desktop app because that’s the easiest way to learn about GitHub. "],["workflow.html", "Workflow Starting up a Project Code Documentation Documenting Data Closing out a Project", " Workflow This workflow section will cover a lot of expectations in pretty high detail and may be slightly overwhelming. The first section covers the immediate steps that you need to know as you go out, so feel free to start there and come back to this over time. If you find anything unclear feel free to edit this in the repository. Starting up a Project Make sure you have access to everything you need: Duke Box (talk to Kay) and the Duke-EJ-Lab GitHub, the VM’s (talk to Anne or Kay, may take a little while) if you are working with protected data or need lots of computational power; make sure you have all the software you need installed: Github Desktop, R, and RStudio or Python and your prefered IDE. We use Github Desktop in this walkthrough for ease, but you’re welcome to use git in command line - whatever fits your needs! Create a GitHub repo to store all your work in that’s within the Duke-EJ-Lab, based off the template. At the reproducible_project_template page, hit the green “Use this template” button and choose a repository name that’s descriptive of your project, separating words with “_“. Set it to private while the work is in progress, so only other lab members can see the repository. Get a local version of your GitHub repo. In GitHub Desktop, find “File -&gt; Clone Repository”, which will bring up a list of repos you could have locally. Select the GitHub repo you just created (you may need to hit the refresh button if you just created it) and hit the blue “Clone” button at the bottom right. This will create a folder containing all the example structure files in your local Documents/GitHub folder, you can make any edits to the documents in those folders. Create a data folder inside the projects folder of the Duke Box with the same name as you chose for your GitHub repository. This is going to hold all the data that is unique to your project - that covers any data sources that aren’t likely to be helpful for future projects or any of your processed data. It’s recommended you follow something similar to the structure shown here, where you separate the raw and clean data within your project folder. Add the data you need to your Box project folder. Check the lab data bank to see if we have anything already cleaned that you can use - you can load anything useful directly from the clean folder, no need to copy it to your own folder. For data you add, add documentation on that data to your Box. It’s most important to document the raw data that you download, so that anyone coming in to your code at a later date will be able to understand the context of the data, know how to update it, and be able to find documentation from the data originator. Add to the README in your Box folder at a minimum when the data was downloaded, how to obtain the data (a link to download/an email for who to contact to get access), and what the data means (a data dictionary) if the data dictionary is not readily accessible either in your folder or at the link to download the data. Here’s an example of of what the README might look like: a layout of the folder structure, followed by the when/how/what of each file. This is an iterative process! Every time you need new data, you’ll come back to update the data documentation. Write some code. Make sure to save even the exploratory data analysis (EDA) work you’re doing - there’s an eda folder in your GitHub repository that you can save out intermediate files in. Commit your code to GitHub. Any time you make a change to your code, commit the change to the online version of the code. It’s especially important to do this very often if you are collaborating with others. Good times to commit are: if you just solved a problem you’ve been working on, or if you’re going to stop actively working on it (eg going to class, working on a paper). To commit, go to GitHub Desktop and add a commit message that explains what you’re adding, then hit the blue “Commit” button on the bottom left. Once you’ve done that, hit the black “Push origin” button on the top right. Comment, document, and iterate! Go back and forth between writing code, committing it, and cleaning up your code + folder structure as you work through the project - it’s inevitable you’ll be trying different methods, different data sources, finding issues that makes you go through old work, and you want to keep the documentation as clean and minimal as you can throughout the process. Keep any analysis that informed your ultimate decisions and make sure it’s commented, but drop code and data that’s no longer relevant. Code Documentation Commenting Every code file should start with something that looks like this and contains: the creator who else is working/has worked on this file when it was last edited a one or two sentence description of what the file is trying to do what the output of the file is. ######################################################################################## # Created by: Anne Driscoll # Other contributors: Chris Timmins, Kay Jowers # Last edited on: 8/31/22 # # This file creates a cleaned dataset of all US flights that is used throughout # the rest of the analysis in this project. # # In the cleaned data each observation is an airport-year, and gives total number of # flights, along with taxi time and several delay metrics. ######################################################################################## If the file is tackling more than one task at a time, it helps to break up the sections for readability, so that others can follow what the code is doing. That might look something like this: # Load data -------------------------------------------------------------------- # Filter data to only TX airports ---------------------------------------------- # Plot number of flights over time --------------------------------------------- Blocks of code should include more detailed commenting throughout, so that your collaborators can follow your code without spending too much time looking through the details of your code and the functions you call - reduce the mental load! The most important kinds of comments are those that explain why you’re implementing something. A quick explanation of the universe of observations you are aiming for, a mention of the theoretical basis for why you’re defining a variable a specific way, can be really useful so people understand the research decisions you’re making. File Naming Good examples would be process_pm_data.R, identify_rural_homes.R, or create_figure_2.R. Bad examples would be CAFO2trylm.R, analysis.R, or cleaning up the obs from corelogic w rural designation.R. Most of the work we do will be a bunch of ordered files, and these should be numbered. Using the examples above, they should be named in order - 01_process_pm_data.R and 02_create_figure_2.R (assuming no files in between!). These can always be changed if you need to add steps in between! Object naming Variable and function names should be lowercase, and use an underscore to separate words. Try to be concise! Good examples would be day_one or first_day, bad examples would be first_day_in_the_data or dotm1. This is largely lifted from Hadley Wickham’s style guide. Additional guidance can be found there, but these are the most important parts for us. Documenting Data Ever piece of data in the raw folder needs to have documented the what, when, how of the data. Closing out a Project There’s lots of ways of re-organizing your code at the end of a project, and you should do whatever works best for you. That said, this is a system that works and feel free to use it. Make sure to read the sections on contributing to the data bank and contributing to the lab package, since those are things to do regardless of your re-organizing process. Starting with your final model or panel, and figure out what data you actually ended up using. Frequently, throughout the research process we’ll build variables multiple ways and try data from different sources as we build our understanding of what the data means. Once you have your final model and are heading towards writing up a report, you don’t need to keep the legacy code and data. Work through the data cleaning code, keeping in mind what is included in the final model. For every data cleaning/combining/munging file make sure that it’s still a necessary step and comment the code with a mindset of 30,000 feet. Make sure each file: Has a header that explains what the overall goal of the file is and how it fits in the project. Is either short enough that it is only doing one thing, or is sectioned off so a reader can tell what steps are happening just by reading section headers. Has comments explaining any non-obvious functions. Has comments explaining research choices. (think: why does it make sense to set the indicator variable to 0 if another variable is NA? not: why is important to account for PM2.5 when studying asthma outcomes) Make sure every data file in your project folder is documented following when/how/what: when the data was downloaded, how to obtain the data (a link to download/an email for who to contact to get access), and what the data means (a data dictionary) if the data dictionary is not readily accessible either in your folder or at the link to download the data. Remove data that is no longer relevant to the project. Now that you’ve gone through all steps of your code and have a good conceptual sense of what is there, think if anything might generalize to other lab projects. If you have functions that might be generally useful, move them to the lab.tools package, as described in the package section and make sure your code is pulling the function from the package. If there’s a really generalizable dataset that others might be able to use, move it to the lab data bank (ie the clean and raw folders of the Box, rather than your personal project folder). Data must be well documented, and the code to clean it must also be saved following the process described in the databank section below. Delete (or archive if you want to be safe) all the processed data in your project folder and re-run your code straight from the downloaded data. Make sure your results haven’t changed. Congrats! You should now have a project that can be run by anyone with access to your GitHub repo and the Duke Box folder. To make sure someone on another computer can use your code, check if there’s someone in lab can try to re-run your code. It’s common that issues come up (even when we’re being really careful) so try to account for the fact that it will likely take a while to de-bug some issues, rather than just quickly hitting run on each file. Make sure the person can dedicate at least a few hours to it. Contributing to the lab data bank If you have a dataset that’s of interest for the rest of the lab and you want to add it to the data bank there’s only a few steps to follow! Write the documentation for the raw data following the following format. Add it to the data section of this document, making sure to follow the instructions in the contributing to the guide section. #### path/to/data {-} **Cleaned formats**: This is a great place to load in the data and print out the first few rows so people can get a sense of what the data shows. Include a description of what an observation is. **What**: What&#39;s the meaning of the data? What does it measure? Are there any known issues or things to be aware of if you&#39;re going to use it? Format: csv? raster? shapefile? excel? Spatial Resolution + Coverage: What geographic region does it cover and what&#39;s the geographic unit that a row represents? blockgroup? city? the entire country? a 0.01° section? Temporal Resolution + Coverage: What time does the data cover? Is it a yearly average? daily? Average over 2006-2020? **Where**: A link to where the data is sourced. The data in the raw folder should be directly downloadable from where you link to, with no changes to it. If the data isn&#39;t publicly available, give info of who you reached out to and the exact format they specified upon giving it to you. Be as precise as you can! **When**: When did you download the data? This is important in case a new release comes out. Once the data documentation is all there, you can move the raw file to the Box. Clean up the code you used to create the “clean” version of the data. Make sure to: include comments describing the methods of cleaning it directly reads the data from the newly saved location it directly saves the data to the Box/clean folder, create a relevant subfolder if you’re going to save several files from the same source data, or save in an existing subfolder if it feels correct. Name the file informatively and put the code in the general_cleaning GitHub repo. Commit and push your code. Run the code! If you’ve checked that the paths match the Box location, it should just pop up Contributing to the lab R package The lab R package is currently minimal, but contains some functions that are useful to some frequent data cleaning tasks that come up across lab projects. If you’re writing a block of code that does a similar task over and over, consider if that’s something that might be useful to just call as a function on your data, and feel free to add it to the lab package! If you do, add a bullet to the lab package section of this document! A step by step guide on how to contribute to the lab R package can be found in the package repo itself! Contributing to (this!) lab onboarding doc Find the GitHub repo that contains all the documents for this site, and get it locally. Install the bookdown package, as well as the packages used in the book (formatR, readr, raster, and tidyverse) install.packages(&quot;bookdown&quot;) library(bookdown) Make changes to whichever of the Rmd files you wanted to update. The site displays html files that are created from the Rmd files, so you need to make sure the html files reflect your changes. Use the bookdown package to render the book. Make sure you’re in the directory containing the Rmd files when you run this. render_book() Commit (and push) your changes, so they are stored on the GitHub website instead of just your local computer, and the changes will update automatically. Beware it can take a while for changes to show up. "],["data.html", "Data Library Documentation format Environmental Data Social Data Other", " Data Library Documentation format Cleaned formats: What: Format: Spatial Resolution + Coverage: Temporal Resolution + Coverage: Where: When: Environmental Data RSEI The Risk Screening Environmental Indicators (RSEI) is a measure of exposure to toxic chemicals from the EPA. The data helps “explore data on releases of toxic substances from industrial and federal facilities. RSEI incorporates information from the Toxics Release Inventory (TRI) on the amount of toxic chemicals released, together with factors such as the chemical’s fate and transport through the environment, each chemical’s relative toxicity, and potential human exposure.” Using the repoted chemical releases combined with the fate and transport models lets us know where people are exposed to those chemical releases. raw/rsei/chemical_data_rsei_v2310.csv Cleaned formats: NA What: “The chemical table contains data [on which are] chemicals reported to TRI, including toxicity, physico-chemical properties, and flag fields to facilitate user selections. The chemical table is also available in EasyRSEI.” Data dictionary Where: Download When: 09/26/22 raw/rsei/facility_data_rsei_v2310.csv Cleaned formats: NA What: “The facility table contains data for reporting facilities, including location, stack parameters and discharge reach, and is also available in EasyRSEI.” Data dictionary Where: Download When: 09/26/22 raw/rsei/blockgroup Cleaned formats: Processed versions of the RSEI data exist at the blockgroup, tract, and county levels. Because of reporting requirements, if you’re using RSEI in any timeseries you need to drop certain observations so that there aren’t huge jumps in years where reporting requirements change - currently data is only processed for the consistent set of chemicals that exists starting in 2011. There is code in the general_cleaning GitHub that would allow you to clean for other year sets in case your year is prior to 2011. Blockgroup data: ## geoid toxconc ## 1 010010201001 421.9514 ## 2 010010201002 336.7555 ## 3 010010202001 382.7406 ## 4 010010202002 506.1840 ## 5 010010203001 430.1625 ## 6 010010203002 568.2655 Tract data: ## geoid toxconc ## 1 01001020100 373.7711 ## 2 01001020200 430.2520 ## 3 01001020300 468.3784 ## 4 01001020400 506.8952 ## 5 01001020500 525.0578 ## 6 01001020600 925.0146 County data: ## geoid toxconc ## 1 01001 361.0561 ## 2 01003 5585.0642 ## 3 01005 154.2646 ## 4 01007 262.4001 ## 5 01009 5077.8284 ## 6 01011 243.6445 What: Format: csv. Each observation in the data is a spill of a specific chemical in a specific blockgroup, with accompanying information on the part of the spill relevant to the blockgroup. Important to consider that each row is NOT the entirety of a spill, NOT the entirety of the toxicity in the blockgroup and NOT the entirety of the impact of a facility, though you can aggregate to investigate spills, blockgroups or facilities. This documentation is helpful in thinking through what the data is and what is an appropriate interpretation. Spatial Resolution + Coverage: Each observation is the impact at the blockgroup level, for all US blockgroups. Temporal Resolution + Coverage: Each file is named censusmicroblockgroup2020_[YYYY].csv, where the YYYY indicates the year of coverage. It can only be interpreted at the year level, there’s no way to break up further temporally. Where: Download When: Downloaded 10/9/2022. PM 2.5 raw/pm25/V4NA03 Cleaned formats: What: Format: raster Spatial Resolution + Coverage: The PM 2.5 data that lives in the Box raw folder is in a raster format, one of the finest resolution datasets (0.01° × 0.01°) built for PM 2.5. The data we have pre-loaded is specifically for North America, and only covers the contiguous 48, if you need Alaska and Hawaii, or international data you’ll need to download their Global/Regional Estimates (V5.GL.02). Temporal Resolution + Coverage: The PM 2.5 data that lives in Box currently covers 2018-2000 and is at a yearly average level. Where: Information on the different products can be found here. We have V4.NA.03 data. When: Downloaded 10/9/2022. Social Data InfoUSA Corelogic Other US boundary shapefiles "],["tools-to-know-about.html", "Tools to know about Basic packages to know about (R) Cool packages to know about (R) Cool functions to know about (R) Basic packages to know about (Python) Cool packages to know about (Python) Cool functions to know about (Python) Other random cool stuff", " Tools to know about Basic packages to know about (R) data.table: hard to read, but faster on average data manipulation raster: work with raster data rmarkdown: the R notebook interface, useful for saving EDA with explanation or for reports, but generally not used for heavy lifting final code. sf: spatial data, more flexible sp: spatial data, more built in options tidyverse dplyr: “a grammar of data manipulation” ggplot2: quick and easy graphics lubridate: easier date/time formats readr: fast and easy way to read flat files stringr: common string manipulations tidyr: reshaping data Cool packages to know about (R) bookdown: lets you build websites and “books” really easily (how this site is built). caret: streamlined model building (Classification And REgression Training) that handles data splitting, pre processing, feature selection, and model tuning and selection. conflicted: alternate way of dealing with package naming conflicts. Great if you’ve ever gotten an error like Unable to find an inherited method for function ‘select’ for signature ‘\"data.frame\"’ because you recently loaded a package that covers a function you commonly use. datapasta: if you ever copy and pasta data from an online table that you later use in your research you should be using this. It lets you copy and paste data directly into an R object, that can then be saved into your reproducible workflow. exactextractr: helps you use rasters and shapefiles to figure out the values in the raster within the shapefile area. Gives lots of control over how you extract - aggregating functions, weighting each raster cell, etc. feather: a binary data format that is language agnostic with high write and read performance. patchwork: lets you arrange several ggplot plots into table like configurations. pbapply: the classic lapply function, but provides a progress bar! profvis: gives you a graphical representation of how quickly each line of a block of code is running. A great tool when something is running slower than expected but you don’t really know where it’s going wrong. ranger: fast random forests! reticulate: the R interface to Python. roxygen: critical for package development. Learn it by contributing to the lab package! rvest: scrape the internet! sqldf: another way to manipulate data in R. sometimes useful since it uses SQLite as the underlying database, so for large data it is usually much faster than straight R. Great for an initial read-in filter. tidycensus: work with Census data directly from your R code. You give it the variable titles and years you want and it pulls it from the Census API. Great for reproducibility! tidytext: text mining wesanderson: color palletes based on Wes Anderson’s movies, some of them make some really beautiful plots. (Personally I love Zissou1, Darjeeling1 and GrandBudapest1 -Anne) validate: great to do checks on your final panel before modeling, makes it easy to “check whether data lives up to expectations you have based on domain knowledge”. A great thing to build into a final code package - also helps people who come back to your code what to expect. vroom: frequently the fastest way to read data in to R. readr has a throughput of 44.02 MB/sec, while vroom reaches 1.23 GB/sec. Our lab package Our lab package: an R package with a mismash of random helpful or fun functions. A sample of functions are listed below. Anyone can add to the package, and it’s great to learn how to build a package!! Any useful functions you stumble upon can be added here (but make sure to include credit to the creator). * detach_all_packages: quickly wipes the packages loaded to your R environment, useful for ensuring reproducibility (since the packages you have loaded impact how things run!) * beep_alert: a function that tells you aloud that your code has finished running. Useful when you have things running just long enough that you want to go look at your email or something. * merge_verbose: wraps the base merge function, but spits out some info on your merge to the console. * some extra ggplot themes Cool functions to know about (R) file.path(): a function that lets you reference paths without building the path manually (eg user/anne/data/testing vs file.path(\"user\", \"anne\", \"data\", \"testing\")), which is great because it means your code won’t break when it crosses to a different OS. Hmisc::describe(): sometimes more useful version of summary to peek into a dataset. It adds the number of distinct observations, as well as directly showing the highest and lowest observations. dplyr::complete(): a function that can help making implicit NA’s explicit. BAMMtools::getJenksBreaks(): great for plotting, given a vector it returns the “Jenks” breaks for cloropleth maps (commonly used in many GIS systems). Basically just returns a list of numbers that you can pass to a mapping function that specify where color changes should happen. Basic packages to know about (Python) NumPy: numerical computing, random number generation, etc Pandas: flexible data structures, data wrangling and cleaning Matplotlib: data visualization scikit-learn: classification, regression, clustering, and some cleaning Cool packages to know about (Python) BeautifulSoup: scraping Cool functions to know about (Python) Other random cool stuff How do we think about EDA and pipelines? blog personal R packages "],["general-stumbling-blocks.html", "General Stumbling Blocks Theoretical Stumbling Blocks Data Cleaning Stumbling Blocks Shapefile Stumbling Blocks", " General Stumbling Blocks Theoretical Stumbling Blocks Test/Train temporal? Data Leakage generally Data Cleaning Stumbling Blocks fail early and often - build in explicit data validation w/ validate::check_that Aggregating - what’s the best function? What are you trying to capture? Do you need to account for area? Do you need to account for population? Do you only care about aggregating over spaces that meet certain conditions? leading 0’s implicit NA’s make sure to use census block group/tract/county FIPS codes for a correct + consistent year Codes change from year to year, so if you use the 2010 set in one part of your code and the 2020 set in another part of your code you’ll drop data that you want to have, and add NA’s that are “false”. Shapefile Stumbling Blocks Projections If you have shapefiles in different projections, you may get unexpected (and incorrect) results when you extract or aggregate. Extracting accurately Extraction functions can reach pretty different answers depending on their methods! Is the function you’re using extracting only the pixel that represents the centroid of your shapfile? Is the function you’re using extracting only pixels that your shapefile completely covers? Is your shapefile weighting by the area of the pixel that your shapefile covers? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
